import bs4
import urllib
import requests
import multiprocessing  # For Performance
import time # For check Performance

print(f"[ì •ë³´] multiprocessing.cpu_count() = {multiprocessing.cpu_count()}\n[ì •ë³´] time = {time.time()}\n[ì •ë³´] findNews.py Imported")

def getNewsItem (p: int, url: str, scraping_news_count: int, items: dict) -> list:
    print("[ì‹œì‘] getNewsItem() í•¨ìˆ˜ #############################")
    print(f"[ì •ë³´] p = {p}, url = {url}, scraping_news_count = {scraping_news_count}, items = {items}")
    item = []  # [response][items][item]ì— ë“¤ì–´ê°ˆ ë¦¬ìŠ¤íŠ¸
    html = bs4.BeautifulSoup(requests.get(url).text, "html.parser")  # urllib vs requests -> https://bentist.tistory.com/44
    tbody = html.find_all("tbody")
    newsList = tbody[0].find_all("tr")
    scrapRange = scraping_news_count; cnt = 0; item = []
    while cnt < scrapRange:
        newsNum = newsList[cnt].find_all('td')[0].text
        if newsNum == "ê³µì§€":
            scrapRange += 1
            cnt += 1
            continue
        newsContent = newsList[cnt].find_all('td', {"class": "ta_l"})[0].text.replace("\r","").replace("\n","").replace("\t","").replace("\xa0","").strip()
        newsLink = "https://www.gnu.ac.kr/main/na/ntt/selectNttInfo.do" + "?nttSn=" + str(newsList[cnt].find_all('a')[0].get('data-id'))
        # news = bs4.BeautifulSoup(urllib.request.urlopen(newsLink), "html.parser")
        news = bs4.BeautifulSoup(requests.get(newsLink).text, "html.parser")  # urllib -> requests ë³€ê²½ (Almost 2x Performance)
        newsDescription = news.find_all("tr", {"class":"cont"})[0].text.replace("\r","").replace("\n","").replace("\t","").replace("\xa0","").strip()[0:40]
        # print(f"[ì •ë³´] newsList = {newsList[0].find_all('td')[1].text}")
        # print(f"[ì •ë³´] newsNum{cnt} = {newsNum}")
        # print(f"[ì •ë³´] newsContent{cnt} = {newsContent}")
        # print(f"[ì •ë³´] newsLink{cnt} = {newsLink}")
        # print(f"[ì •ë³´] newsDescription{cnt} = {newsDescription}")
        temp = {
            "title": newsContent,
            "description": newsDescription
        }
        # print(f"[ì •ë³´] pid = {p} ######################## \ntitle = {newsContent}\ndescription = {newsDescription}\n")
        item.append(temp)
        cnt += 1
    # items[p] = item
    # items.put(item)
    items.append(item)
    print(f"[ì¢…ë£Œ] getNewsItem() í•¨ìˆ˜ pid= {p} #############################")
    return 0
    
    



def findNews (scraping_news_count: int = 2) -> dict:
    print("[ì‹œì‘] findNews() í•¨ìˆ˜ #############################")
    starttime = time.time()
    # News previewë¥¼ ì¼ì¼íˆ ë‹¤ ë“¤ì–´ê°€ì„œ íŒŒì‹±í•˜ë‹¤ë³´ë‹ˆ í•„ì—°ì ìœ¼ë¡œ êµ‰ì¥íˆ ëŠë ¤ì§..
    # ìºì‹œ ì‹œìŠ¤í…œì„ ë„ì…í•´ì•¼í• ë“¯
    # ì•„ë‹ˆë©´ ë­”ê°€ ë‹¤ë¥¸ê±°ë¼ë„... ìˆë‹¤ë©´?
    
    
    
    itemslst = []
    # category = ["ê¸°ê´€", "í•™ì‚¬", "ì¥í•™"]  # DEVELOPING...
    urls = ["https://www.gnu.ac.kr/main/na/ntt/selectNttList.do?bbsId=1028&mi=1126","https://www.gnu.ac.kr/main/na/ntt/selectNttList.do?bbsId=1029&mi=1127","https://www.gnu.ac.kr/main/na/ntt/selectNttList.do?bbsId=1075&mi=1376"]  # ê¸°ê´€, í•™ì‚¬, ì¥í•™ ê³µì§€ì‚¬í•­ì˜ url
    # = Multiprocessingì„ ì´ìš©í•œ ë³‘ë ¬ì²˜ë¦¬ =  ##  ğŸ“https://seing.tistory.com/92
    manager = multiprocessing.Manager()   # ğŸ“https://seing.tistory.com/92
    # for i in range(3):
    #     itemslst["items{0}".format(i)] = manager.list()  # í”„ë¡œì„¸ìŠ¤ ê°„ì— ê³µìœ í•  ë”•ì…”ë„ˆë¦¬ ìƒì„±
    item0 = manager.list()
    item1 = manager.list()
    item2 = manager.list()
    
    
    # news_pool = multiprocessing.Pool(processes=3)  # [ê¸°ê´€, í•™ì‚¬, ì¥í•™]ì˜ 3ê°œ ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë‹ˆ 3ê°œì˜ í”„ë¡œì„¸ìŠ¤ë¥¼ ìƒì„±
    # news_pool.map(getNewsItem, [0, 1, 2], urls, [scraping_news_count, scraping_news_count, scraping_news_count], [items, items, items])
    # q = multiprocessing.Queue()  # íë¥¼ ì´ìš©í•œ ë³‘ë ¬ì²˜ë¦¬ë¥¼ í•˜ë ¤ í–ˆìœ¼ë‚˜, íëŠ” FIFO. ì¦‰, ì„ ì…ì„ ì¶œ ë°©ì‹ì˜ ìë£Œêµ¬ì¡°ì¸ë°, í”„ë¡œì„¸ìŠ¤ê°€ ëë‚˜ëŠ” ìˆœì„œê°€ ë‹¬ë¼ì„œ ì ìš©ì´ ì•ˆë¨.
    # worker = []
    
    
    
    
    # for i in range(3):
    #     P = multiprocessing.Process(target=getNewsItem, args=(i,urls[i], scraping_news_count ,items))
    #     worker.append(P)
    #     P.start()

    # for process in worker:
    #     process.join()  # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•´ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ í›„ ì¢…ë£Œ
    
    
    P0 = multiprocessing.Process(target=getNewsItem, args=(0,urls[0], scraping_news_count ,item0))
    P1 = multiprocessing.Process(target=getNewsItem, args=(1,urls[1], scraping_news_count ,item1))
    P2 = multiprocessing.Process(target=getNewsItem, args=(2,urls[2], scraping_news_count ,item2))
    
    P0.start();P1.start();P2.start()
    P0.join();P1.join();P2.join()
    
    print("[ì •ë³´] itemlst <- item0, item1, item2")
    itemslst.append(list(item0));itemslst.append(list(item1));itemslst.append(list(item2))
        
    
    # for i in range(3):  íì— ì ìš©ëë˜ ì½”ë“œ
    #     itemslst.append(items.get())
    
    # for i in range(3):
    #     print(f'[ì •ë³´] itemslst[{i}] = { itemslst["items{0}".format(i)] }')
    # print(f"[ì •ë³´] itemslst = {itemslst}")
    
    # items.close() íì— ì ìš©ëë˜ ì½”ë“œ
    # items.join_thread()  íì— ì ìš©ëë˜ ì½”ë“œ
    
    # print("///////////////////////////////////////////")
    
    
    # for i in range(3):
        # html = bs4.BeautifulSoup(urllib.request.urlopen(urls[i]), "html.parser")  # https://itsaessak.tistory.com/295
        # tbody = html.find_all("tbody")
        # newsList = tbody[0].find_all("tr")
        # scrapRange = scraping_news_count; cnt = 0; item = []
        # while cnt < scrapRange:
        #     newsNum = newsList[cnt].find_all('td')[0].text
        #     if newsNum == "ê³µì§€":
        #         scrapRange += 1
        #         cnt += 1
        #         continue
        #     newsContent = newsList[cnt].find_all('td', {"class": "ta_l"})[0].text.strip()
        #     newsLink = "https://www.gnu.ac.kr/main/na/ntt/selectNttInfo.do" + "?nttSn=" + str(newsList[cnt].find_all('a')[0].get('data-id'))
        #     news = bs4.BeautifulSoup(urllib.request.urlopen(newsLink), "html.parser")
        #     newsDescription = news.find_all("tr", {"class":"cont"})[0].text.strip()[0:40]
        #     # print(f"[ì •ë³´] newsList = {newsList[0].find_all('td')[1].text}")
        #     # print(f"[ì •ë³´] newsNum{cnt} = {newsNum}")
        #     # print(f"[ì •ë³´] newsContent{cnt} = {newsContent}")
        #     # print(f"[ì •ë³´] newsLink{cnt} = {newsLink}")
        #     # print(f"[ì •ë³´] newsDescription{cnt} = {newsDescription}")
        #     temp = {
        #         "title": newsContent,
        #         "description": newsDescription
        #     }
        #     item.append(temp)
        #     cnt += 1
        # # print(f"[ì •ë³´] while end, scraping_news_count = {scraping_news_count}")
        
        
    # for i in range(scraping_news_count):
    #     title = tbody[0].find_all("a")[i].text
    #     link = "https://www.gnu.ac.kr" + tbody[0].find_all("a")[i].get("href")
    #     print(f"[ì •ë³´] title = {title}")
    #     print(f"[ì •ë³´] link = {link}")
    #     news.append({"title": title, "link": link})
    
    
    response = [
                    {
                        "carousel": {
                        "type": "listCard",
                        "items": 
                    [
                        {
                            "header": 
                            {
                                "title": "ê³µì§€ - ê¸°ê´€ (1/3)"
                            },
                                "items": itemslst[0],
                                "buttons": 
                                [
                                    {
                                        "action":  "webLink",
                                        "label": "ë”ë³´ê¸°",
                                        "webLinkUrl": urls[0]
                                    }
                                ]
                            },
                            {
                            "header": 
                            {
                                "title": "ê³µì§€ - í•™ì‚¬ (2/3)"
                            },
                            "items": itemslst[1],
                                "buttons": 
                                [
                                    {
                                        "action":  "webLink",
                                        "label": "ë”ë³´ê¸°",
                                        "webLinkUrl": urls[1]
                                    }
                                ]
                            },
                            {
                            "header": 
                            {
                                "title": "ê³µì§€ - ì¥í•™ (3/3)"
                            },
                            "items": itemslst[2],
                                "buttons": 
                                [
                                    {
                                        "action":  "webLink",
                                        "label": "ë”ë³´ê¸°",
                                        "webLinkUrl": urls[2]
                                    }
                                ]
                            }
                    ]
                }
            }
        ]

    
    
    endtime = time.time()
    print(f"[ì •ë³´] {__name__} ì‹¤í–‰ì‹œê°„ = {endtime - starttime}ì´ˆ")
    print("[ì¢…ë£Œ] findNews() í•¨ìˆ˜ #########")
    return response








if __name__ == "__main__":
    # Local TEST environment
    # campus = "ê°€ì¢Œìº í¼ìŠ¤"
    # restaurant = "ê°€ì¢Œ êµì§ì›ì‹ë‹¹"
    # date = "ì˜¤ëŠ˜"
    print("FINDNEWS #########################")
    starttime = time.time()
    print(findNews())
    endtime = time.time()
    print(f"[ì •ë³´] {__name__} ì‹¤í–‰ì‹œê°„ = {endtime - starttime}ì´ˆ")